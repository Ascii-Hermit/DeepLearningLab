{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "import zipfile\n",
    "import urllib.request\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "data_dir = 'cats_and_dogs_filtered'\n",
    "\n",
    "if not os.path.exists(data_dir):\n",
    "    print(\"Downloading Cats and Dogs dataset...\")\n",
    "    dataset_url = \"https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip\"\n",
    "    dataset_path = \"cats_and_dogs_filtered.zip\"\n",
    "    \n",
    "    urllib.request.urlretrieve(dataset_url, dataset_path)\n",
    "\n",
    "    with zipfile.ZipFile(dataset_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall()\n",
    "\n",
    "    print(f\"Dataset extracted to {data_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.7911473153129457\n",
      "Epoch [2/10], Loss: 0.6938958499166701\n",
      "Epoch [3/10], Loss: 0.6931854968979245\n",
      "Epoch [4/10], Loss: 0.6936405698458353\n",
      "Epoch [5/10], Loss: 0.6933690527128795\n",
      "Epoch [6/10], Loss: 0.6933544051079523\n",
      "Epoch [7/10], Loss: 0.6931953770773751\n",
      "Epoch [8/10], Loss: 0.6931847362291246\n",
      "Epoch [9/10], Loss: 0.6932553762481326\n",
      "Epoch [10/10], Loss: 0.6932744478422498\n",
      "With L2 norm:\n",
      "Epoch [1/10], Loss: 0.7710752468260508\n",
      "Epoch [2/10], Loss: 0.769323450232309\n",
      "Epoch [3/10], Loss: 0.7567859159575568\n",
      "Epoch [4/10], Loss: 0.7429817744663784\n",
      "Epoch [5/10], Loss: 0.7400355745875646\n",
      "Epoch [6/10], Loss: 0.7343502148749337\n",
      "Epoch [7/10], Loss: 0.728412303659651\n",
      "Epoch [8/10], Loss: 0.7240960881823585\n",
      "Epoch [9/10], Loss: 0.7202976582542299\n",
      "Epoch [10/10], Loss: 0.7166589271454584\n"
     ]
    }
   ],
   "source": [
    "#question 1\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(128 * 32 * 32, 512)  # Adjusted for 256x256 image size\n",
    "        self.fc2 = nn.Linear(512, 2)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.conv1(x)))\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "        x = self.pool(torch.relu(self.conv3(x)))\n",
    "        x = x.view(-1, 128 * 32 * 32)  \n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),  \n",
    "    transforms.ToTensor(),         \n",
    "])\n",
    "\n",
    "train_dir = os.path.join(data_dir, 'train')\n",
    "train_dataset = datasets.ImageFolder(root=train_dir, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "model = SimpleCNN().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.01) \n",
    "\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()  \n",
    "        outputs = model(inputs)  \n",
    "        loss = criterion(outputs, labels)  \n",
    "        loss.backward()  \n",
    "        optimizer.step()  \n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}], Loss: {running_loss/len(train_loader)}\")\n",
    "\n",
    "print(\"With L2 norm:\")\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0.0\n",
    "    l2_lambda = 0.01  \n",
    "    for inputs, labels in train_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        l2_reg = 0\n",
    "        for param in model.parameters():\n",
    "            l2_reg += torch.norm(param, 2)  \n",
    "        \n",
    "        loss += l2_lambda * l2_reg  \n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}], Loss: {running_loss/len(train_loader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with L1 regularization (Optimizer)\n",
      "Epoch [1/10], Loss: 165.04669213673426\n",
      "Epoch [2/10], Loss: 90.22653452555339\n",
      "Epoch [3/10], Loss: 89.63100348578558\n",
      "Epoch [4/10], Loss: 89.74948422870939\n",
      "Epoch [5/10], Loss: 89.58445921398345\n",
      "Epoch [6/10], Loss: 89.67096092587425\n",
      "Epoch [7/10], Loss: 89.81757693820529\n",
      "Epoch [8/10], Loss: 89.5268781147306\n",
      "Epoch [9/10], Loss: 89.5874763367668\n",
      "Epoch [10/10], Loss: 89.68427034408327\n"
     ]
    }
   ],
   "source": [
    "#question 2\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(128 * 32 * 32, 512)  \n",
    "        self.fc2 = nn.Linear(512, 2)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.conv1(x)))\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "        x = self.pool(torch.relu(self.conv3(x)))\n",
    "        x = x.view(-1, 128 * 32 * 32)  \n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),  \n",
    "    transforms.ToTensor(),          \n",
    "])\n",
    "\n",
    "train_dir = os.path.join(data_dir, 'train')\n",
    "train_dataset = datasets.ImageFolder(root=train_dir, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "model = SimpleCNN().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)  \n",
    "\n",
    "print(\"Training with L1 regularization (Optimizer)\")\n",
    "l1_lambda = 0.01  \n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()  \n",
    "        outputs = model(inputs)  \n",
    "        loss = criterion(outputs, labels)  \n",
    "\n",
    "        l1_reg = 0\n",
    "        for param in model.parameters():\n",
    "            l1_reg += torch.sum(torch.abs(param))  \n",
    "        \n",
    "        loss += l1_lambda * l1_reg  \n",
    "        \n",
    "        loss.backward() \n",
    "        optimizer.step()  \n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}], Loss: {running_loss/len(train_loader)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with Dropout Regularization (Dropout rate = 0.5):\n",
      "Epoch [1/10], Train Loss: 0.9103, Validation Loss: 0.6932\n",
      "Epoch [2/10], Train Loss: 0.6934, Validation Loss: 0.6931\n",
      "Epoch [3/10], Train Loss: 0.6932, Validation Loss: 0.6932\n",
      "Epoch [4/10], Train Loss: 0.6934, Validation Loss: 0.6932\n",
      "Epoch [5/10], Train Loss: 0.6932, Validation Loss: 0.6931\n",
      "Epoch [6/10], Train Loss: 0.6932, Validation Loss: 0.6932\n",
      "Epoch [7/10], Train Loss: 0.6933, Validation Loss: 0.6931\n",
      "Epoch [8/10], Train Loss: 0.6931, Validation Loss: 0.6931\n",
      "Epoch [9/10], Train Loss: 0.6934, Validation Loss: 0.6933\n",
      "Epoch [10/10], Train Loss: 0.6933, Validation Loss: 0.6932\n",
      "\n",
      "Training without Dropout Regularization:\n",
      "Epoch [1/10], Train Loss: 0.8885, Validation Loss: 0.6938\n",
      "Epoch [2/10], Train Loss: 0.6904, Validation Loss: 0.6912\n",
      "Epoch [3/10], Train Loss: 0.6814, Validation Loss: 0.6919\n",
      "Epoch [4/10], Train Loss: 0.6736, Validation Loss: 0.6876\n",
      "Epoch [5/10], Train Loss: 0.6531, Validation Loss: 0.6997\n",
      "Epoch [6/10], Train Loss: 0.6128, Validation Loss: 0.7495\n",
      "Epoch [7/10], Train Loss: 0.5531, Validation Loss: 0.7958\n",
      "Epoch [8/10], Train Loss: 0.4733, Validation Loss: 0.9047\n",
      "Epoch [9/10], Train Loss: 0.4080, Validation Loss: 1.1222\n",
      "Epoch [10/10], Train Loss: 0.3173, Validation Loss: 1.3571\n"
     ]
    }
   ],
   "source": [
    "#question 3\n",
    "\n",
    "class SimpleCNNWithDropout(nn.Module):\n",
    "    def __init__(self, dropout_rate=0.5):\n",
    "        super(SimpleCNNWithDropout, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(128 * 32 * 32, 512)  \n",
    "        self.fc2 = nn.Linear(512, 2)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout_rate) if dropout_rate > 0 else nn.Identity()  \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.conv1(x)))\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "        x = self.pool(torch.relu(self.conv3(x)))\n",
    "        x = x.view(-1, 128 * 32 * 32) \n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout(x)  \n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),  \n",
    "    transforms.ToTensor(),          \n",
    "])\n",
    "\n",
    "train_dir = os.path.join(data_dir, 'train')\n",
    "val_dir = os.path.join(data_dir, 'validation')\n",
    "\n",
    "train_dataset = datasets.ImageFolder(root=train_dir, transform=transform)\n",
    "val_dataset = datasets.ImageFolder(root=val_dir, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "def train_model(dropout_rate=0.0, epochs=10):\n",
    "    model = SimpleCNNWithDropout(dropout_rate=dropout_rate).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()  \n",
    "            outputs = model(inputs) \n",
    "            loss = criterion(outputs, labels) \n",
    "\n",
    "            loss.backward() \n",
    "            optimizer.step()  \n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        val_loss /= len(val_loader)  \n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Train Loss: {running_loss/len(train_loader):.4f}, \"\n",
    "              f\"Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "print(\"Training with Dropout Regularization (Dropout rate = 0.5):\")\n",
    "train_model(dropout_rate=0.5)\n",
    "\n",
    "print(\"\\nTraining without Dropout Regularization:\")\n",
    "train_model(dropout_rate=0.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with Custom Dropout Regularization:\n",
      "Epoch [1/10], Loss: 0.8477671392380245\n",
      "Epoch [2/10], Loss: 0.6873678583947439\n",
      "Epoch [3/10], Loss: 0.6882958904145255\n",
      "Epoch [4/10], Loss: 0.6904015446466113\n",
      "Epoch [5/10], Loss: 0.6718009663006616\n",
      "Epoch [6/10], Loss: 0.6560739458553375\n",
      "Epoch [7/10], Loss: 0.6306019141560509\n",
      "Epoch [8/10], Loss: 0.5971986788605886\n",
      "Epoch [9/10], Loss: 0.5232475333743625\n",
      "Epoch [10/10], Loss: 0.47641435880509636\n"
     ]
    }
   ],
   "source": [
    "# question 4\n",
    "\n",
    "class CustomDropout(nn.Module):\n",
    "    def __init__(self, dropout_rate=0.5):\n",
    "        super(CustomDropout, self).__init__()\n",
    "        self.dropout_rate = dropout_rate\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if self.training:  \n",
    "            # Bernoulli distribution: 1 - dropout_rate gives us the probability of \"keeping\" a neuron\n",
    "            mask = torch.bernoulli(torch.full(x.shape, 1 - self.dropout_rate, device=x.device))\n",
    "            x = x * mask \n",
    "            x = x / (1 - self.dropout_rate)  \n",
    "        return x\n",
    "\n",
    "class SimpleCNNWithCustomDropout(nn.Module):\n",
    "    def __init__(self, dropout_rate=0.5):\n",
    "        super(SimpleCNNWithCustomDropout, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(128 * 32 * 32, 512)  \n",
    "        self.fc2 = nn.Linear(512, 2)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.custom_dropout = CustomDropout(dropout_rate) \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.conv1(x)))\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "        x = self.pool(torch.relu(self.conv3(x)))\n",
    "        x = x.view(-1, 128 * 32 * 32)  \n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.custom_dropout(x)  \n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "class SimpleCNNWithLibraryDropout(nn.Module):\n",
    "    def __init__(self, dropout_rate=0.5):\n",
    "        super(SimpleCNNWithLibraryDropout, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(128 * 32 * 32, 512)  \n",
    "        self.fc2 = nn.Linear(512, 2)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.dropout = nn.Dropout(dropout_rate)  \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.conv1(x)))\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "        x = self.pool(torch.relu(self.conv3(x)))\n",
    "        x = x.view(-1, 128 * 32 * 32)  \n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout(x)  \n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),  \n",
    "    transforms.ToTensor(),         \n",
    "])\n",
    "\n",
    "train_dir = os.path.join(data_dir, 'train')\n",
    "train_dataset = datasets.ImageFolder(root=train_dir, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "def train_model(model):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    epochs = 10\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()  \n",
    "            outputs = model(inputs)  \n",
    "            loss = criterion(outputs, labels)  \n",
    "\n",
    "            loss.backward()  \n",
    "            optimizer.step()  \n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {running_loss/len(train_loader)}\")\n",
    "\n",
    "print(\"Training with Custom Dropout Regularization:\")\n",
    "model_with_custom_dropout = SimpleCNNWithCustomDropout(dropout_rate=0.5).to(device)\n",
    "train_model(model_with_custom_dropout)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with Early Stopping:\n",
      "Epoch [1/10], Train Loss: 1.3746, Validation Loss: 0.6885\n",
      "Epoch [2/10], Train Loss: 0.6825, Validation Loss: 0.6852\n",
      "Epoch [3/10], Train Loss: 0.6404, Validation Loss: 0.6602\n",
      "Epoch [4/10], Train Loss: 0.5381, Validation Loss: 0.6940\n",
      "Epoch [5/10], Train Loss: 0.3652, Validation Loss: 0.7292\n",
      "Epoch [6/10], Train Loss: 0.1760, Validation Loss: 0.8892\n",
      "Epoch [7/10], Train Loss: 0.0691, Validation Loss: 1.1959\n",
      "Epoch [8/10], Train Loss: 0.0378, Validation Loss: 1.5693\n",
      "Early stopping triggered.\n"
     ]
    }
   ],
   "source": [
    "#questions 5\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(64*64*64, 512)\n",
    "        self.fc2 = nn.Linear(512, 2)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.conv1(x)))\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 64*64*64)  \n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),  \n",
    "    transforms.ToTensor(),          \n",
    "])\n",
    "\n",
    "train_dir = os.path.join(data_dir, 'train')\n",
    "val_dir = os.path.join(data_dir, 'validation')\n",
    "train_dataset = datasets.ImageFolder(root=train_dir, transform=transform)\n",
    "val_dataset = datasets.ImageFolder(root=val_dir, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, min_delta=0.01):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.best_loss = np.inf\n",
    "        self.counter = 0\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if val_loss < self.best_loss - self.min_delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "\n",
    "def train_with_early_stopping(model, criterion, optimizer, epochs=20):\n",
    "    early_stopping = EarlyStopping(patience=5, min_delta=0.01)\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()  \n",
    "            outputs = model(inputs)  \n",
    "            loss = criterion(outputs, labels) \n",
    "\n",
    "            loss.backward()  \n",
    "            optimizer.step()  \n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        val_loss /= len(val_loader)  \n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Train Loss: {running_loss/len(train_loader):.4f}, \"\n",
    "              f\"Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "        early_stopping(val_loss)\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "model_with_early_stopping = SimpleCNN().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model_with_early_stopping.parameters(), lr=0.001)\n",
    "\n",
    "print(\"Training with Early Stopping:\")\n",
    "train_with_early_stopping(model_with_early_stopping, criterion, optimizer, epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
