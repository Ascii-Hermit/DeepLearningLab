{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from torchvision import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Test accuracy on FashionMNIST: 11.36%\n",
      "Epoch 1/5, Loss: 0.4200, Accuracy: 86.97%\n",
      "Epoch 2/5, Loss: 0.2697, Accuracy: 90.09%\n",
      "Epoch 3/5, Loss: 0.2267, Accuracy: 91.08%\n",
      "Epoch 4/5, Loss: 0.1966, Accuracy: 91.37%\n",
      "Epoch 5/5, Loss: 0.1709, Accuracy: 91.93%\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x1500 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#question1\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,)) \n",
    "])\n",
    "\n",
    "train_dataset = datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "print(\"cpu\")\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(7*7*64, 128) \n",
    "        self.fc2 = nn.Linear(128, 10)      \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = torch.max_pool2d(x, 2)\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = torch.max_pool2d(x, 2)\n",
    "        x = x.view(-1, 7*7*64)  \n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "model = SimpleCNN()\n",
    "\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()  \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    accuracy = correct / total\n",
    "    return accuracy\n",
    "\n",
    "accuracy = evaluate_model(model, test_loader)\n",
    "print(f\"Test accuracy on FashionMNIST: {accuracy * 100:.2f}%\")\n",
    "\n",
    "def fine_tune_model(model, train_loader, test_loader, epochs=5):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "        accuracy = evaluate_model(model, test_loader)\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {running_loss/len(train_loader):.4f}, Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "fine_tune_model(model, train_loader, test_loader, epochs=5)\n",
    "\n",
    "def show_predictions(model, test_loader, num_images=5):\n",
    "    model.eval()\n",
    "    data_iter = iter(test_loader)\n",
    "    inputs, labels = next(data_iter)\n",
    "    outputs = model(inputs)\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "    _, axes = plt.subplots(1, num_images, figsize=(15, 15))\n",
    "    for i in range(num_images):\n",
    "        axes[i].imshow(inputs[i].squeeze(), cmap='gray')\n",
    "        axes[i].set_title(f\"Pred: {predicted[i].item()}, True: {labels[i].item()}\")\n",
    "        axes[i].axis('off')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "show_predictions(model, test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: 'cats_and_dogs_filtered\\\\train'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 18\u001b[0m\n\u001b[0;32m      2\u001b[0m transform \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m: transforms\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[0;32m      4\u001b[0m         transforms\u001b[38;5;241m.\u001b[39mRandomResizedCrop(\u001b[38;5;241m224\u001b[39m),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     14\u001b[0m     ]),\n\u001b[0;32m     15\u001b[0m }\n\u001b[0;32m     17\u001b[0m data_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcats_and_dogs_filtered\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m---> 18\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mdatasets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mImageFolder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m val_dataset \u001b[38;5;241m=\u001b[39m datasets\u001b[38;5;241m.\u001b[39mImageFolder(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(data_dir, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalidation\u001b[39m\u001b[38;5;124m'\u001b[39m), transform[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     21\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m DataLoader(train_dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\torchvision\\datasets\\folder.py:328\u001b[0m, in \u001b[0;36mImageFolder.__init__\u001b[1;34m(self, root, transform, target_transform, loader, is_valid_file, allow_empty)\u001b[0m\n\u001b[0;32m    319\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m    320\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    321\u001b[0m     root: Union[\u001b[38;5;28mstr\u001b[39m, Path],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    326\u001b[0m     allow_empty: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    327\u001b[0m ):\n\u001b[1;32m--> 328\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    329\u001b[0m \u001b[43m        \u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    330\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    331\u001b[0m \u001b[43m        \u001b[49m\u001b[43mIMG_EXTENSIONS\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mis_valid_file\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    332\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    333\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget_transform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_transform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    334\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_valid_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_valid_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    335\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_empty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_empty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    336\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    337\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msamples\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\torchvision\\datasets\\folder.py:149\u001b[0m, in \u001b[0;36mDatasetFolder.__init__\u001b[1;34m(self, root, loader, extensions, transform, target_transform, is_valid_file, allow_empty)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m    139\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    140\u001b[0m     root: Union[\u001b[38;5;28mstr\u001b[39m, Path],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    146\u001b[0m     allow_empty: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    147\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    148\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(root, transform\u001b[38;5;241m=\u001b[39mtransform, target_transform\u001b[38;5;241m=\u001b[39mtarget_transform)\n\u001b[1;32m--> 149\u001b[0m     classes, class_to_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_classes\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroot\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    150\u001b[0m     samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmake_dataset(\n\u001b[0;32m    151\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot,\n\u001b[0;32m    152\u001b[0m         class_to_idx\u001b[38;5;241m=\u001b[39mclass_to_idx,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    155\u001b[0m         allow_empty\u001b[38;5;241m=\u001b[39mallow_empty,\n\u001b[0;32m    156\u001b[0m     )\n\u001b[0;32m    158\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader \u001b[38;5;241m=\u001b[39m loader\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\torchvision\\datasets\\folder.py:234\u001b[0m, in \u001b[0;36mDatasetFolder.find_classes\u001b[1;34m(self, directory)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfind_classes\u001b[39m(\u001b[38;5;28mself\u001b[39m, directory: Union[\u001b[38;5;28mstr\u001b[39m, Path]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[List[\u001b[38;5;28mstr\u001b[39m], Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mint\u001b[39m]]:\n\u001b[0;32m    208\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Find the class folders in a dataset structured as follows::\u001b[39;00m\n\u001b[0;32m    209\u001b[0m \n\u001b[0;32m    210\u001b[0m \u001b[38;5;124;03m        directory/\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;124;03m        (Tuple[List[str], Dict[str, int]]): List of all classes and dictionary mapping each class to an index.\u001b[39;00m\n\u001b[0;32m    233\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 234\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfind_classes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\torchvision\\datasets\\folder.py:41\u001b[0m, in \u001b[0;36mfind_classes\u001b[1;34m(directory)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfind_classes\u001b[39m(directory: Union[\u001b[38;5;28mstr\u001b[39m, Path]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[List[\u001b[38;5;28mstr\u001b[39m], Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mint\u001b[39m]]:\n\u001b[0;32m     37\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Finds the class folders in a dataset.\u001b[39;00m\n\u001b[0;32m     38\u001b[0m \n\u001b[0;32m     39\u001b[0m \u001b[38;5;124;03m    See :class:`DatasetFolder` for details.\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 41\u001b[0m     classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(entry\u001b[38;5;241m.\u001b[39mname \u001b[38;5;28;01mfor\u001b[39;00m entry \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscandir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m entry\u001b[38;5;241m.\u001b[39mis_dir())\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m classes:\n\u001b[0;32m     43\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find any class folder in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdirectory\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: 'cats_and_dogs_filtered\\\\train'"
     ]
    }
   ],
   "source": [
    "#question2\n",
    "transform = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ]),\n",
    "}\n",
    "\n",
    "data_dir = 'cats_and_dogs_filtered'\n",
    "train_dataset = datasets.ImageFolder(os.path.join(data_dir, 'train'), transform['train'])\n",
    "val_dataset = datasets.ImageFolder(os.path.join(data_dir, 'validation'), transform['val'])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "model = models.alexnet(pretrained=True)\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "model.classifier[6] = nn.Linear(model.classifier[6].in_features, 2)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = optim.Adam(model.classifier.parameters(), lr=0.001)\n",
    "\n",
    "epochs = 5\n",
    "\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, epochs=5):\n",
    "    best_acc = 0.0\n",
    "    for epoch in range(epochs):\n",
    "        model.train()  \n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        model.eval()  \n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        accuracy = correct / total\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss/len(train_loader):.4f}, Accuracy: {accuracy*100:.2f}%\")\n",
    "\n",
    "        if accuracy > best_acc:\n",
    "            best_acc = accuracy\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "\n",
    "train_model(model, train_loader, val_loader, criterion, optimizer, epochs)\n",
    "\n",
    "model.eval()  \n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in val_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = correct / total\n",
    "print(f\"Final Accuracy on the validation set: {accuracy*100:.2f}%\")\n",
    "\n",
    "def visualize_predictions(model, val_loader):\n",
    "    model.eval()\n",
    "    data_iter = iter(val_loader)\n",
    "    inputs, labels = next(data_iter)\n",
    "    outputs = model(inputs.to(device))\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "    fig, axes = plt.subplots(1, 5, figsize=(15, 15))\n",
    "    for i in range(15):\n",
    "        axes[i].imshow(inputs[i].permute(1, 2, 0).cpu().numpy())\n",
    "        axes[i].set_title(f\"Pred: {'Dog' if predicted[i] == 1 else 'Cat'}, True: {'Dog' if labels[i] == 1 else 'Cat'}\")\n",
    "        axes[i].axis('off')\n",
    "\n",
    "    plt.show()\n",
    "visualize_predictions(model, val_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 0.2740, Accuracy: 95.50%\n",
      "New best accuracy: 95.50%\n",
      "Checkpoint saved to best_model.pth.tar\n",
      "Checkpoint saved to checkpoint_epoch_1.pth.tar\n",
      "Epoch 2/5, Loss: 0.2117, Accuracy: 95.20%\n",
      "Checkpoint saved to checkpoint_epoch_2.pth.tar\n",
      "Epoch 3/5, Loss: 0.1964, Accuracy: 95.90%\n",
      "New best accuracy: 95.90%\n",
      "Checkpoint saved to best_model.pth.tar\n",
      "Checkpoint saved to checkpoint_epoch_3.pth.tar\n",
      "Epoch 4/5, Loss: 0.1957, Accuracy: 95.80%\n",
      "Checkpoint saved to checkpoint_epoch_4.pth.tar\n",
      "Epoch 5/5, Loss: 0.1857, Accuracy: 95.00%\n",
      "Checkpoint saved to checkpoint_epoch_5.pth.tar\n"
     ]
    }
   ],
   "source": [
    "#question3\n",
    "def save_checkpoint(state, filename=\"checkpoint.pth.tar\"):\n",
    "    \"\"\"Saves the checkpoint to a file.\"\"\"\n",
    "    torch.save(state, filename)\n",
    "    print(f\"Checkpoint saved to {filename}\")\n",
    "\n",
    "\n",
    "def train_model_with_checkpoint(model, train_loader, val_loader, criterion, optimizer, num_epochs=5, checkpoint_interval=1):\n",
    "    best_acc = 0.0 \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  \n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        model.eval()  \n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        accuracy = correct / total\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader):.4f}, Accuracy: {accuracy*100:.2f}%\")\n",
    "\n",
    "        if accuracy > best_acc:\n",
    "            best_acc = accuracy\n",
    "            print(f\"New best accuracy: {accuracy*100:.2f}%\")\n",
    "            save_checkpoint({\n",
    "                'epoch': epoch + 1,\n",
    "                'state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': running_loss / len(train_loader),\n",
    "                'best_acc': best_acc\n",
    "            }, filename=\"best_model.pth.tar\")\n",
    "        \n",
    "        if (epoch + 1) % checkpoint_interval == 0:\n",
    "            save_checkpoint({\n",
    "                'epoch': epoch + 1,\n",
    "                'state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': running_loss / len(train_loader),\n",
    "                'best_acc': best_acc\n",
    "            }, filename=f\"checkpoint_epoch_{epoch + 1}.pth.tar\")\n",
    "\n",
    "\n",
    "data_dir = 'cats_and_dogs_filtered'\n",
    "\n",
    "transform = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ]),\n",
    "}\n",
    "\n",
    "train_dataset = datasets.ImageFolder(os.path.join(data_dir, 'train'), transform['train'])\n",
    "val_dataset = datasets.ImageFolder(os.path.join(data_dir, 'validation'), transform['val'])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "model = models.alexnet(pretrained=True)\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "model.classifier[6] = nn.Linear(model.classifier[6].in_features, 2)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.classifier.parameters(), lr=0.001)\n",
    "\n",
    "train_model_with_checkpoint(model, train_loader, val_loader, criterion, optimizer, num_epochs=5, checkpoint_interval=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
